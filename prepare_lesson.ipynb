{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34685688",
   "metadata": {},
   "source": [
    "# Parsing Text (aka Prepping Text Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf1ec1",
   "metadata": {},
   "source": [
    "What is it?\n",
    "- Breaking our text data into smaller compenents and reduce variability between words\n",
    "\n",
    "Why do we care? \n",
    "- Allows us to better understand our data programatically and get us ready for explore and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2a6d2",
   "metadata": {},
   "source": [
    "Workflow\n",
    "\n",
    "original text--->\n",
    "1. lowercase text\n",
    "2. remove accented and non-ASCII characters\n",
    "3. remove special characters\n",
    "4. tokenize the strings into discrete units\n",
    "5. stem/lemmatize words\n",
    "6. remove stopwords\n",
    "\n",
    "ready for exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7c252",
   "metadata": {},
   "source": [
    "## Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177fb50",
   "metadata": {},
   "source": [
    "### original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = \"Paul Erdős and George Pólya were influential Hungarian mathematicians who contributed \\\n",
    "a lot to the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), \\\n",
    "but is often incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b9a0d",
   "metadata": {},
   "source": [
    "### 1. lowercase text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91138f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0609841c",
   "metadata": {},
   "source": [
    "### 2. remove any accented characters and non-ASCII characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0003d",
   "metadata": {},
   "source": [
    "- `unicodedata.normalize` removes any inconsistencies in unicode character encoding\n",
    "- `.encode` to convert the resulting string to the ASCII character set\n",
    "- `.decode` to turn the resulting bytes object back into a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32753e69",
   "metadata": {},
   "source": [
    "Use `unicodedata.normalize().encode().decode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3070e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db40fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f6eed33",
   "metadata": {},
   "source": [
    "### 3. remove special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fcc61",
   "metadata": {},
   "source": [
    "- remove anything that isn't a-z, a number, a single quote, or a whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadfa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expression operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc12d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use re.sub to remove special characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54588fc0",
   "metadata": {},
   "source": [
    "### 4. tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d02af",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking something down into smaller, discrete units. These units are called tokens.\n",
    "\n",
    "It's common to tokenize the strings to break up words and punctutation left over into discrete units. \n",
    "\n",
    "Use `nltk.tokenize.ToktokTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import natural language toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbadf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac95470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1742c1",
   "metadata": {},
   "source": [
    "### 5. stem or lemmatize words (choose one!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219029f",
   "metadata": {},
   "source": [
    "Stemming\n",
    "- **truncates** words to their \"stem\"\n",
    "- algorithmic rules (non lingustic)\n",
    "- example: \"calls\", \"called\", \"calling\" --> \"call\"\n",
    "- fast and efficient\n",
    "\n",
    "\n",
    "Lemmatize\n",
    "- **changes** words to their \"root\"\n",
    "- it can conjugate to the base word \n",
    "- example: \"mouse\", \"mice\" --> \"mouse\"\n",
    "- slower than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f67eb",
   "metadata": {},
   "source": [
    "#### stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982f095",
   "metadata": {},
   "source": [
    "Use `nltk.porter.PorterStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create porter stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efa3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76322cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use stemmer - apply stem to each word in our string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join words back together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b774e07",
   "metadata": {},
   "source": [
    "#### lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1772d",
   "metadata": {},
   "source": [
    "Use `nltk.stem.WordNetLemmatizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f729f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the first time\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2437772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca5d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a74ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use lemmatize - apply stem to each word in our string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae7377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join words back together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c918a833",
   "metadata": {},
   "source": [
    "### 6. remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055beaaa",
   "metadata": {},
   "source": [
    "Words which have little or no significance, especially when constructing meaningful features from text, are known as stopwords\n",
    "- example: a, an, the, and like\n",
    "\n",
    "We will use a standard English language stopwords list from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44778e",
   "metadata": {},
   "source": [
    "Use `nltk.corpus.stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopwords list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e450dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need to do once\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad6714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b1605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a list to remove some stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b35d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87046b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84eb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eebf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d208d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split words in lemmatized article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8689d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add to stopword list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from list of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01341d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show how many words we removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join words back together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7dffe",
   "metadata": {},
   "source": [
    "#### ready for exploration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bddfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa7535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227c7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
